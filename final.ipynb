{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Final processing of the pipeline\n",
    "import pandas as pd\n",
    "data = pd.read_csv('hatefulmemes.csv')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def get_image_paths(directory):\n",
    "    # Define the patterns for the file types we're interested in\n",
    "    patterns = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n",
    "\n",
    "    # Use a list comprehension to get a list of all image paths\n",
    "    image_paths = [path.split(\"\\\\\")[-1] for pattern in patterns for path in glob.glob(os.path.join(directory, pattern))]\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "# Assume `directory` is the path to your directory of images\n",
    "image_paths = get_image_paths('D:/ASU/Data Mining/Project experimentation/Mask-RCNN-TF2/all_images/')\n",
    "data['img'] = data['img'].apply(lambda x: x.split('/')[-1])\n",
    "\n",
    "data = data[data['img'].isin(image_paths)]\n",
    "data = data.iloc[:1000,:]\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from mrcnn import utils\n",
    "from mrcnn.config import Config\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import cv2\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "class InferenceConfig(Config):\n",
    "    NAME = \"coco_inference\"\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 1 + 80  # COCO dataset has 80 classes + one background class\n",
    "    IMAGE_MIN_DIM = 800\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "\n",
    "config = InferenceConfig()\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # Load the image\n",
    "    \n",
    "    image = cv2.imread(f\"D:/ASU/Data Mining/Project experimentation/Mask-RCNN-TF2/all_images//{image_path.split('/')[-1]}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Resize the image\n",
    "    image, window, scale, padding, crop = utils.resize_image(\n",
    "        image,\n",
    "        min_dim=config.IMAGE_MIN_DIM,\n",
    "        min_scale=config.IMAGE_MIN_SCALE,\n",
    "        max_dim=config.IMAGE_MAX_DIM,\n",
    "        mode=config.IMAGE_RESIZE_MODE)\n",
    "\n",
    "    # Mold the image (subtract the mean pixel value)\n",
    "    molded_image = modellib.mold_image(image, config)\n",
    "\n",
    "    # Add dimension\n",
    "    molded_image = np.expand_dims(molded_image, 0)\n",
    "    # Create the image metadata\n",
    "    image_meta = modellib.compose_image_meta(\n",
    "        image_id=0, \n",
    "        original_image_shape=image.shape, \n",
    "        image_shape=image.shape, \n",
    "        window=window, \n",
    "        scale=scale, \n",
    "        active_class_ids=np.zeros([config.NUM_CLASSES], dtype=np.int32))\n",
    "\n",
    "    # Add dimension\n",
    "    image_meta = np.expand_dims(image_meta, 0)\n",
    "    backbone_shapes = modellib.compute_backbone_shapes(config, config.IMAGE_SHAPE)\n",
    "    anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES, \n",
    "                                            config.RPN_ANCHOR_RATIOS,\n",
    "                                            backbone_shapes,\n",
    "                                            config.BACKBONE_STRIDES, \n",
    "                                            config.RPN_ANCHOR_STRIDE)\n",
    "\n",
    "    # Match the anchors to the image shape\n",
    "    anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)\n",
    "\n",
    "    return molded_image, image_meta, anchors\n",
    "\n",
    "\n",
    "def clean_text_with_nltk(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join the cleaned words back into a string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "data[\"clean_text\"] = data[\"text\"].apply(clean_text_with_nltk)\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "# Set all layers of BERT to not trainable\n",
    "for layer in model.layers:\n",
    "    \n",
    "    layer.trainable = False\n",
    "\n",
    "def get_text_embedding(text, model, tokenizer):\n",
    "    text_tokens = tokenizer(text, padding='max_length', truncation=True, return_tensors=\"tf\", max_length=128)\n",
    "    return np.reshape(model(text_tokens['input_ids'])[0][:, 0, :].numpy(), (768))\n",
    "\n",
    "def preprocessing_image(image_path):\n",
    "    molded_image, image_meta, anchors = load_and_preprocess_image(image_path.split('\\\\')[-1])\n",
    "    return molded_image, image_meta, anchors\n",
    "\n",
    "def preprocess_text(cleaned_text):\n",
    "    return get_text_embedding(cleaned_text, model=model, tokenizer=tokenizer)\n",
    "\n",
    "def data_generator(df, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch_df = df.iloc[i:i+batch_size]\n",
    "            \n",
    "            molded_images = []\n",
    "            image_metadatas = []\n",
    "            anchors = []\n",
    "            text_data = []\n",
    "            \n",
    "            for _, row in batch_df.iterrows():\n",
    "                molded_image, image_metadata, anchor = preprocessing_image(row[\"img\"])\n",
    "                text_datum = preprocess_text(row[\"clean_text\"])\n",
    "                \n",
    "                molded_images.append(molded_image[0])\n",
    "                image_metadatas.append(image_metadata[0])\n",
    "                anchors.append(anchor)\n",
    "                text_data.append(text_datum)\n",
    "            \n",
    "            molded_images = np.array(molded_images)\n",
    "            image_metadatas = np.array(image_metadatas)\n",
    "            anchors = np.squeeze(np.array(anchors), axis=1)\n",
    "            text_data = np.array(text_data)\n",
    "            yield [molded_images, image_metadatas, anchors, text_data], batch_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "test_generator = data_generator(test_df, batch_size=1)\n",
    "train_generator = data_generator(train_df, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrcnn_model = modellib.MaskRCNN(mode=\"inference\", model_dir=os.getcwd(), config=config)\n",
    "mrcnn_model.load_weights('mask_rcnn_coco.h5', by_name=True)\n",
    "# Print the names of all layers in the model\n",
    "# Freeze the layers of the Mask R-CNN model\n",
    "for layer in mrcnn_model.keras_model.layers:\n",
    "    layer.trainable = False\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Concatenate, Input\n",
    "\n",
    "# Assume `model` is your original Mask R-CNN model\n",
    "\n",
    "# Get the output of the FPN layers\n",
    "fpn_layer_names = ['fpn_p2', 'fpn_p3', 'fpn_p4', 'fpn_p5', 'fpn_p6']\n",
    "fpn_outputs = [mrcnn_model.keras_model.get_layer(name).output for name in fpn_layer_names]\n",
    "\n",
    "# Apply Global Average Pooling to each FPN output\n",
    "pooled_outputs = [GlobalAveragePooling2D()(output) for output in fpn_outputs]\n",
    "\n",
    "# Concatenate the pooled outputs\n",
    "concatenated_outputs = Concatenate()(pooled_outputs)\n",
    "\n",
    "# Project the concatenated outputs into a 768-dimensional vector\n",
    "outputs = Dense(768)(concatenated_outputs)\n",
    "\n",
    "text_inputs = Input(shape=(768,)\n",
    ")\n",
    "\n",
    "cancatenation_image_text = Concatenate()([outputs, text_inputs])\n",
    "binary_classification = Dense(1, activation='sigmoid')(cancatenation_image_text)\n",
    "\n",
    "# Create the final model\n",
    "\n",
    "\n",
    "\n",
    "image_model = Model(inputs=mrcnn_model.keras_model.input+[text_inputs], outputs=binary_classification)\n",
    "image_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib._GeneratorContextManager at 0x28b52b46ac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "myconfig = ConfigProto()\n",
    "myconfig.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=myconfig)\n",
    "session.as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "800/800 [==============================] - 1484s 2s/step - loss: 2.8605 - accuracy: 0.5888 - val_loss: 1.2875 - val_accuracy: 0.6100\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 1467s 2s/step - loss: 0.8138 - accuracy: 0.6100 - val_loss: 0.9700 - val_accuracy: 0.6200\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 1471s 2s/step - loss: 0.8449 - accuracy: 0.5975 - val_loss: 1.9617 - val_accuracy: 0.6200\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 1471s 2s/step - loss: 1.1302 - accuracy: 0.6050 - val_loss: 0.7012 - val_accuracy: 0.6500\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 1479s 2s/step - loss: 1.0170 - accuracy: 0.6025 - val_loss: 4.8544 - val_accuracy: 0.6150\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 1510s 2s/step - loss: 1.3166 - accuracy: 0.6075 - val_loss: 0.9550 - val_accuracy: 0.5650\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 1451s 2s/step - loss: 0.9527 - accuracy: 0.6325 - val_loss: 0.7145 - val_accuracy: 0.5550\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 1470s 2s/step - loss: 1.1484 - accuracy: 0.6212 - val_loss: 3.9633 - val_accuracy: 0.6050\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 1613s 2s/step - loss: 1.1230 - accuracy: 0.6062 - val_loss: 2.0976 - val_accuracy: 0.6100\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 1489s 2s/step - loss: 1.0943 - accuracy: 0.6212 - val_loss: 5.4418 - val_accuracy: 0.6100\n"
     ]
    }
   ],
   "source": [
    "validation_steps = len(test_df) // 1\n",
    "\n",
    "history = image_model.fit(\n",
    "    x=train_generator,\n",
    "    steps_per_epoch=len(train_df) // 1,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViLBERTTokenizer, ViLBERTModel\n",
    "\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = ViLBERTTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model\n",
    "model = ViLBERTModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer global_average_pooling2d_1 is trainable\n",
      "Layer global_average_pooling2d_2 is trainable\n",
      "Layer global_average_pooling2d_3 is trainable\n",
      "Layer global_average_pooling2d_4 is trainable\n",
      "Layer global_average_pooling2d_5 is trainable\n",
      "Layer concatenate_1 is trainable\n",
      "Layer dense_3 is trainable\n",
      "Layer concatenate_2 is trainable\n",
      "Layer dense_4 is trainable\n"
     ]
    }
   ],
   "source": [
    "for layer in image_model.layers:\n",
    "    if layer.trainable:\n",
    "        print(f\"Layer {layer.name} is trainable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
